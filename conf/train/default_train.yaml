# @package _group_

# reproducibility
seed: 42

# model name
model_name: xlm-roberta-large-bugged-token-selection # used to name the directory in which model's checkpoints will be stored (experiments/model_name/...)
lr: 1e-5
weight_decay: 0.01
# pl_trainer
pl_trainer:
  _target_: pytorch_lightning.Trainer
  gpus: 1
  accumulate_grad_batches: 2
  # gradient_clip_val: 10.0
  val_check_interval: 1.0  # you can specify an int "n" here => validation every "n" steps
  max_epochs: 10
  # uncomment the lines below for training with mixed precision
  precision: 16
  # amp_level: O2

# early stopping callback
# "early_stopping_callback: null" will disable early stopping
early_stopping_callback:
  _target_: pytorch_lightning.callbacks.EarlyStopping
  monitor: val_loss
  mode: min
  patience: 3

# model_checkpoint_callback
# "model_checkpoint_callback: null" will disable model checkpointing
model_checkpoint_callback:
  _target_: pytorch_lightning.callbacks.ModelCheckpoint
  monitor: val_acc
  mode: max
  verbose: True
  save_top_k: 1
  dirpath: checkpoints/
